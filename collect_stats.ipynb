{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'text_format' from 'google.protobuf' (C:\\Users\\vchad\\AppData\\Roaming\\Python\\Python39\\site-packages\\google\\protobuf\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\vchad\\Documents\\ICT\\aaai_gnns\\collect_stats.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/vchad/Documents/ICT/aaai_gnns/collect_stats.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msummary\u001b[39;00m \u001b[39mimport\u001b[39;00m summary_iterator\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\__init__.py:38\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_typing\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m module_util \u001b[39mas\u001b[39;00m _module_util\n\u001b[0;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyLoader \u001b[39mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     41\u001b[0m \u001b[39m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\__init__.py:37\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[39m# go/tf-wildcard-import\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m pywrap_tensorflow \u001b[39mas\u001b[39;00m _pywrap_tensorflow\n\u001b[1;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m context\n\u001b[0;32m     39\u001b[0m \u001b[39m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[39m# Bring in subpackages.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m data\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\context.py:36\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclient\u001b[39;00m \u001b[39mimport\u001b[39;00m pywrap_tf_session\n\u001b[0;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m cancellation\n\u001b[1;32m---> 36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m execute\n\u001b[0;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m executor\n\u001b[0;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m monitoring\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py:17\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Functions called by the generated code to execute an eager-mode op.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m \u001b[39mimport\u001b[39;00m text_format\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m tensor_pb2\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m pywrap_tfe\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'text_format' from 'google.protobuf' (C:\\Users\\vchad\\AppData\\Roaming\\Python\\Python39\\site-packages\\google\\protobuf\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.summary import summary_iterator\n",
    "\n",
    "#summary_reader = tbx.SummaryReader(\"C:/Users/vchad/Desktop/scout_double_opt_ray_results/\")\n",
    "#tag_data = summary_reader.#get_data(\"scalar\")\n",
    "#print(tag_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind, mannwhitneyu\n",
    "\n",
    "osl = os.listdir\n",
    "ospj = os.path.join"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get relevant runs data given filters of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def natural_sort(l): \n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower() \n",
    "    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)] \n",
    "    return sorted(l, key=alphanum_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "runs_dir = \"/home/derposoft/ray_results\"\n",
    "runs_filters = [\n",
    "    \"fcscout_baseline200h_1r2b_SEED\",\n",
    "    \"gnnscout_200h2.2l_1r2b_SEED\",\n",
    "    \"gatscout_200h2.2l_1r2b_SEED\"\n",
    "]\n",
    "runs_filters = [\n",
    "    \"fcskirmish_5hp_baseline100_50_PAPER_SEED\",\n",
    "    \"gcnskirmish_5hp_baseline100_50_3.4_PAPER_SEED\",\n",
    "    \"gatLNskirmish_5hp_GLOBALbaseline100_50_3.4_PAPER_SEED\",\n",
    "    \"gcnLNskirmish_5hp_baseline100_50_3.4_PAPER_SEED\",\n",
    "]\n",
    "runs_filters = [\n",
    "    \"fcskirmish_5hp_baseline100_50_100ksteps_SEED\",\n",
    "    \"gcnskirmish_5hp_baseline100_50_2.4_PAPER_SEED\",\n",
    "]\n",
    "runs_dirs = [\n",
    "    \"/home/derposoft/Documents/tb_logs/40k_runs/fcskirmish_100_50\",\n",
    "    \"/home/derposoft/Documents/tb_logs/40k_runs/gcnskirmish_100_50_3.4\",\n",
    "]\n",
    "runs_filters = [\n",
    "    \"fcskirmish_5hp_baseline100_50_PAPER_SEED\",\n",
    "    \"gcnskirmish_5hp_baseline100_50_3.4_PAPER_SEED\",\n",
    "]\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "runs_dirs = [\n",
    "    \"/home/derposoft/Documents/tb_logs/seeded_runs/\",\n",
    "    \"/home/derposoft/Documents/tb_logs/tb_logs.2022-12-18_paper/\"\n",
    "]\n",
    "runs_filters_5hp = [\n",
    "    \"baseline_SEED\",\n",
    "    \"hybridGAT_5hp_globalemb_NOOPT_SEED\",\n",
    "    \"hybridGAT_5hp_hybridemb_NOOPT_SEED\",\n",
    "    \"hybridGAT_5hp_localemb_NOOPT_SEED\",\n",
    "    \"hybridGCN_5hp_globalemb_NOOPT_SEED\",\n",
    "    \"hybridGCN_5hp_hybridemb_NOOPT_SEED\",\n",
    "    \"hybridGCN_5hp_localemb_NOOPT_SEED\",\n",
    "    \"hybridGT_5hp_globalemb_NOOPT_SEED\",\n",
    "    \"hybridGT_5hp_hybridemb_NOOPT_SEED\",\n",
    "    \"hybridGT_5hp_localemb_NOOPT_SEED\",\n",
    "]\n",
    "runs_filters_2hp = [\n",
    "    \"baseline_2hp_NOOPT_SEED\",\n",
    "    \"hybridGAT_2hp_localemb_NOOPT_SEED\",\n",
    "    \"hybridGCN_2hp_localemb_NOOPT_SEED\",\n",
    "    \"hybridGT_2hp_localemb_NOOPT_SEED\",\n",
    "]\n",
    "runs_filters_20hp = [\n",
    "    \"baseline_20hp_NOOPT_SEED\",\n",
    "    \"hybridGAT_20hp_localemb_NOOPT_SEED\",\n",
    "    \"hybridGCN_20hp_localemb_NOOPT_SEED\",\n",
    "    \"hybridGT_20hp_localemb_NOOPT_SEED\",\n",
    "]\n",
    "runs_filters_5hp_2v2 = [\n",
    "    \"baseline_5hp_NOOPT_2r2b_SEED\",\n",
    "    \"hybridGAT_5hp_localemb_NOOPT_2r2b_SEED\",\n",
    "    \"hybridGCN_5hp_localemb_NOOPT_2r2b_SEED\",\n",
    "    \"hybridGT_5hp_localemb_NOOPT_2r2b_SEED\",\n",
    "]\n",
    "\"\"\"\n",
    "runs_dirs = [\"/home/vchad/ray_results/\"]\n",
    "runs_filters_scout_2v2 = [\n",
    "    \"fcscout_200h2.2l_2r2b\",\n",
    "    \"gcnscout_200h2.2l_2r2b\",\n",
    "    \"gatscout_200h2.2l_2r2b\",\n",
    "]\n",
    "runs_filters_scout_2v2_OPT = [\n",
    "    \"fcscout_200h2.2l_2r2b_OPT\",\n",
    "    \"gcnscout_200h2.2l_2r2b_OPT\",\n",
    "    \"gatscout_200h2.2l_2r2b_OPT\",\n",
    "]\n",
    "runs_filters = runs_filters_scout_2v2_OPT\n",
    "runs = sum(([ospj(runs_dir, x) for x in osl(runs_dir)] for runs_dir in runs_dirs), [])\n",
    "tot_steps_len = 200\n",
    "runs = natural_sort(runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_filters_runs = [\n",
    "    [run for run in runs if runs_filter in run]\n",
    "    for runs_filter in runs_filters\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_file = \"progress.csv\"\n",
    "run_data_col = \"episode_reward_mean\"\n",
    "# run_data_col = \"evaluation/episode_reward_mean\"\n",
    "# run_data_col = \"ray/tune/episode_reward_mean\"\n",
    "run_step_col = \"timesteps_total\"\n",
    "all_runs_data = []\n",
    "for filter_runs in all_filters_runs:\n",
    "    curr_run_data = []\n",
    "    for run in filter_runs:\n",
    "        datafile = pd.read_csv(ospj(run, run_data_file))\n",
    "        curr_run_data.append(datafile[run_data_col])\n",
    "    all_runs_data.append(curr_run_data)\n",
    "runs_steps = [\n",
    "    pd.read_csv(ospj(run, run_data_file))[run_step_col]\n",
    "    for run in runs\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### given data, run \"map\" on data to get values suitable for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds the first training step when reward exceeds a given value ge\n",
    "def first_ep_to_val(runs, steps, ge=30):\n",
    "    first_eps = []\n",
    "    for run, steps in zip(runs, steps):\n",
    "        found = False\n",
    "        for step, val in zip(steps, run):\n",
    "            if val >= ge:\n",
    "                first_eps.append(step)\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            first_eps.append(-1)\n",
    "    return first_eps\n",
    "\n",
    "# finds max avg reward\n",
    "def max_avg_reward(runs, steps):\n",
    "    max_vals = []\n",
    "    for run in runs:\n",
    "        if len(run) < tot_steps_len: continue\n",
    "        max_vals.append(max(run))\n",
    "    return max_vals\n",
    "\n",
    "# returns last reward\n",
    "def last_ep_reward(runs, steps):\n",
    "    last_vals = []\n",
    "    for run in runs:\n",
    "        if len(run) < tot_steps_len: continue\n",
    "        last_vals.append(run.tolist()[-1])\n",
    "    return last_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_ep_to_30 = [\n",
    "    first_ep_to_val(filter_runs_data, runs_steps)\n",
    "    for filter_runs_data in all_runs_data\n",
    "]\n",
    "max_reward = [\n",
    "    max_avg_reward(filter_runs_data, runs_steps)\n",
    "    for filter_runs_data in all_runs_data\n",
    "]\n",
    "last_reward = [\n",
    "    last_ep_reward(filter_runs_data, runs_steps)\n",
    "    for filter_runs_data in all_runs_data\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### given \"map\" values, run \"reduce\" to aggregate values and show significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretty print a dictionary\n",
    "def pretty_print(d, tabs=0, tabsize=4):\n",
    "    for k in d:\n",
    "        v = d[k]\n",
    "        nspaces = \" \" * tabsize * tabs\n",
    "        if type(v) == dict:\n",
    "            print(f\"{nspaces}{k}:\")\n",
    "            pretty_print(v, tabs+1, tabsize)\n",
    "        else:\n",
    "            print(f\"{nspaces}{k}: {v}\")\n",
    "\n",
    "# get stats for mapped values for a given experiment\n",
    "def get_stats(all_runs_vals, firstn=-1):\n",
    "    stats = {}\n",
    "    #baselines = all_runs_vals[0]\n",
    "    baselines = all_runs_vals[0]\n",
    "    for filter, vals in zip(runs_filters, all_runs_vals):\n",
    "        if firstn > 0: vals = vals[:firstn]\n",
    "        mu = np.mean(vals)\n",
    "        sigma = np.std(vals)\n",
    "        ci95z = 1.96\n",
    "        ttest_results = ttest_ind(baselines, vals, alternative=\"less\", equal_var=False)\n",
    "        stats[filter] = {\n",
    "            \"str\": f\"{mu}+/-{ci95z*sigma} -- p={ttest_results.pvalue}\",\n",
    "            #\"mean\": f\"{mu}+/-{ci95z*sigma}\",\n",
    "            #\"median\": np.median(vals),\n",
    "            #\"std\": sigma,\n",
    "            #\"ci\": [mu-ci95z*sigma, mu+ci95z*sigma],\n",
    "            #\"n\": len(vals),\n",
    "            #\"mannwu\": mannwhitneyu(baselines, vals, alternative=\"less\"),\n",
    "            #\"ttest_less\": ttest_ind(baselines, vals, alternative=\"less\", equal_var=False),\n",
    "            #\"ttest_more\": ttest_ind(baselines, vals, alternative=\"greater\", equal_var=False),\n",
    "        }\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3677444/2837841425.py:22: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  ttest_results = ttest_ind(baselines, vals, alternative=\"less\", equal_var=False)\n"
     ]
    }
   ],
   "source": [
    "firstn = -1\n",
    "first_ep_to_30_stats = get_stats(first_ep_to_30, firstn=firstn)\n",
    "max_reward_stats = get_stats(max_reward, firstn=firstn)\n",
    "last_reward_stats = get_stats(last_reward, firstn=firstn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max reward stats:\n",
      "fcscout_200h2.2l_2r2b:\n",
      "    str: 315.88399999999996+/-97.01067834653254 -- p=0.5\n",
      "gcnscout_200h2.2l_2r2b:\n",
      "    str: 291.36800000000005+/-81.03147943633756 -- p=0.8652437992802904\n",
      "gatscout_200h2.2l_2r2b:\n",
      "    str: 308.45400000000006+/-84.40203387145594 -- p=0.6309763643621769\n",
      "\n",
      "first ep to 30 stats:\n",
      "fcscout_200h2.2l_2r2b:\n",
      "    str: 400.0+/-0.0 -- p=nan\n",
      "gcnscout_200h2.2l_2r2b:\n",
      "    str: 400.0+/-0.0 -- p=nan\n",
      "gatscout_200h2.2l_2r2b:\n",
      "    str: 400.0+/-0.0 -- p=nan\n",
      "\n",
      "last reward stats:\n",
      "fcscout_200h2.2l_2r2b:\n",
      "    str: 310.965+/-87.87791408853536 -- p=0.5\n",
      "gcnscout_200h2.2l_2r2b:\n",
      "    str: 289.265+/-79.53091652216766 -- p=0.8519812674069078\n",
      "gatscout_200h2.2l_2r2b:\n",
      "    str: 305.60800000000006+/-79.4189696561319 -- p=0.6033281625887179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_stat(stats, name=\"\"):\n",
    "    assert name != \"\"\n",
    "    print(name+\" stats:\")\n",
    "    pretty_print(stats)\n",
    "    print()\n",
    "\n",
    "print_stat(max_reward_stats, \"max reward\")\n",
    "print_stat(first_ep_to_30_stats, \"first ep to 30\")\n",
    "print_stat(last_reward_stats, \"last reward\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ict')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "55c685f8cf5c890b69147c020cec2e2dc7dfd76edeb06885af029168509bee56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
